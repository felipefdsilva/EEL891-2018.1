{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["\n<h1 id=\"Aprendizado-de-M\u00e1quina-2018.2\">Aprendizado de M\u00e1quina 2018.2<a class=\"anchor-link\" href=\"#Aprendizado-de-M\u00e1quina-2018.2\">\u00b6</a></h1>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h3 id=\"Trabalho-1---Estimando-o-pre\u00e7o-de-im\u00f3veis-com-t\u00e9cnicas-de-regress\u00e3o-(competi\u00e7\u00e3o-do-Kaggle)\">Trabalho 1 - Estimando o pre\u00e7o de im\u00f3veis com t\u00e9cnicas de regress\u00e3o (competi\u00e7\u00e3o do Kaggle)<a class=\"anchor-link\" href=\"#Trabalho-1---Estimando-o-pre\u00e7o-de-im\u00f3veis-com-t\u00e9cnicas-de-regress\u00e3o-(competi\u00e7\u00e3o-do-Kaggle)\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Aluno: Felipe Ferreira da Silva</p>\n<p>Nome de Usu\u00c3\u00a1rio no Kaggle: <strong>felipefdsilva</strong></p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h3 id=\"Introdu\u00e7\u00e3o\">Introdu\u00e7\u00e3o<a class=\"anchor-link\" href=\"#Introdu\u00e7\u00e3o\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Este trabalho tem como prop\u00c3\u00b3sito a participa\u00c3\u00a7\u00c3\u00a3o na competi\u00c3\u00a7\u00c3\u00a3o Kaggle: \"House Prices: Advanced Regression Techniques\". Mais detalhes sobre a competi\u00c3\u00a7\u00c3\u00a3o podem ser lidos no endere\u00c3\u00a7o: <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a></p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Em resumo, o dataset foi manipulado de forma a:</p>\n<ol>\n<li>remover outliers, </li>\n<li>normalizar a distribui\u00c3\u00a7\u00c3\u00a3o de valores da vari\u00c3\u00a1vel alvo</li>\n<li>normalizar a distribui\u00c3\u00a7\u00c3\u00a3o de preditores </li>\n<li>minimizar o n\u00c3\u00bamero de colunas </li>\n<li>prencher campos vazios (\"NaN\") com o valor m\u00c3\u00a9dio</li>\n<li>tratar vari\u00c3\u00a1veis categ\u00c3\u00b3ricas atrav\u00c3\u00a9s de one hot encoding.</li>\n</ol>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Foram utilizados algumas os regressores:</p>\n<ol>\n<li>LinearRegression()</li>\n<li>RandomForest()</li>\n<li>GradientBoostingRegressor()</li>\n<li>ElasticNet</li>\n</ol>\n<p>Com o \u00c3\u00baltimo apresentando o melhor resultado.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>O modelo foi testado atrav\u00c3\u00a9s de regress\u00c3\u00a3o linear. V\u00c3\u00a1rios gr\u00c3\u00a1ficos s\u00c3\u00a3o exibidos ao longo do relat\u00c3\u00b3rio para ilustrar e justificar as opera\u00c3\u00a7\u00c3\u00b5es realizadas.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>O relat\u00c3\u00b3rio foi escrito com auxilio da ferramenta Jupyter Notebook. Todo o c\u00c3\u00b3digo Python utilizado encontra-se aqui, com coment\u00c3\u00a1rios explicativos para cada c\u00c3\u00a9lula.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h3 id=\"Importa\u00e7\u00e3o-das-bibliotecas-necess\u00e1rias\">Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias<a class=\"anchor-link\" href=\"#Importa\u00e7\u00e3o-das-bibliotecas-necess\u00e1rias\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Foram utilizadas basicamente bibliotecas relacionada a plotagem de gr\u00c3\u00a1ficos, ferramentas matem\u00c3\u00a1ticas e de estat\u00c3\u00adstica, e bibliotecas scikit-learn para manipula\u00c3\u00a7\u00c3\u00a3o do dataset e cria\u00c3\u00a7\u00c3\u00a3o dos modelos.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport pandas as pd\nimport numpy as np\nfrom math import sqrt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Configura\u00c3\u00a7\u00c3\u00b5es para o gr\u00c3\u00a1ficos</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nplt.style.use(style='ggplot')\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Importando-os-dados-e-convertendo-num-dataset-do-pandas\">Importando os dados e convertendo num dataset do pandas<a class=\"anchor-link\" href=\"#Importando-os-dados-e-convertendo-num-dataset-do-pandas\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_file = './train.csv'\ntest_file = './test.csv'\n\ntrain_dataset = pd.read_csv(train_file)\ntest_dataset = pd.read_csv(test_file)\n\ntarget = train_dataset.SalePrice\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Com o c\u00c3\u00b3digo da c\u00c3\u00a9lula abaixo, observa-se a formato dos dataset de treino e teste.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nprint (train_dataset.shape)\nprint (test_dataset.shape)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Observando o conte\u00c3\u00bado de algumas colunas:</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset.head()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id='Estudando-a-variavel-alvo-\"SalePrice\"'>Estudando a variavel alvo <strong>\"SalePrice\"</strong><a class=\"anchor-link\" href='#Estudando-a-variavel-alvo-\"SalePrice\"'>\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsns.distplot(target, color='blue');\nprint(\"Skewness:\", target.skew())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Observa-se pelo gr\u00c3\u00a1fico acima que a vari\u00c3\u00a1vel alvo (SalePrice) tem uma cauda longa para a direita. Modelos de regress\u00c3\u00a3o linear tem melhor comportamento quando trabalham com vari\u00c3\u00a1veis de distribui\u00c3\u00a7\u00c3\u00a3o normal. Portanto, fazemos uma trasforma\u00c3\u00a7\u00c3\u00a3o logar\u00c3\u00adtimica para aproximar SalePrice de uma gaussiana.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntarget = np.log(target)\nsns.distplot(target, color='blue');\nprint(\"Skewness: \", target.skew())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Lidando-com-vari\u00e1veis-com-campos-vazios\">Lidando com vari\u00e1veis com campos vazios<a class=\"anchor-link\" href=\"#Lidando-com-vari\u00e1veis-com-campos-vazios\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Observa-se com o c\u00c3\u00b3digo abaixo as caracter\u00c3\u00adsticas que possuem mais campos vazios no dataset, ordenadas de cima para baixo de acordo com o percentual de dados ausentes.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntotal = train_dataset.isnull().sum().sort_values(ascending=False)\npercent = (train_dataset.isnull().sum()/train_dataset.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Escolhe-se descartar todas as colunas que possuem mais de 15% de dados faltando, por considerar este um valor suficientemente alto para desconsiderar o potencial preditivo dessas caracter\u00c3\u00adsticas.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset = train_dataset.drop((missing_data[missing_data['Total'] > 81]).index, axis=1)\ntest_dataset = test_dataset.drop((missing_data[missing_data['Total'] > 81]).index, axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Lidando-com-vari\u00e1veis-num\u00e9ricas\">Lidando com vari\u00e1veis num\u00e9ricas<a class=\"anchor-link\" href=\"#Lidando-com-vari\u00e1veis-num\u00e9ricas\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nnumeric_features = train_dataset.select_dtypes(include=[np.number])\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Um vez destacada a parcela do dataset que cont\u00c3\u00a9m colunas de dados num\u00c3\u00a9ricas, estuda-se a correla\u00c3\u00a7\u00c3\u00a3o destas colunas com a variavel alvo.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ncorr = numeric_features.corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[1:])\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>De acordo com a descri\u00c3\u00a7\u00c3\u00a3o do dataset, \"MSSubClass\" \u00c3\u00a9 uma vari\u00c3\u00a1vel categ\u00c3\u00b3rica e n\u00c3\u00a3o num\u00c3\u00a9rica. Portanto, precisa de um cuidado especial. Por enquanto, ser\u00c3\u00a1 retirada do dataset numeric_features.</p>\n<p>Tamb\u00c3\u00a9m ser\u00c3\u00a3o retiradas as colunas 'SalePrice' e 'Id', para realiza\u00c3\u00a7\u00c3\u00a3o de plotagens.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nnumeric_features = numeric_features.drop(['SalePrice', 'MSSubClass','Id'], axis=1)\nnumeric_features_list = numeric_features.columns\nnumeric_features.shape\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Com os gr\u00c3\u00a1ficos abaixo, observa-se a distribui\u00c3\u00a7\u00c3\u00a3o das medidas de cada vari\u00c3\u00a1vel, com o objetivo de encontrar outliers (pontos isolados que destoam da tend\u00c3\u00aancia da popula\u00c3\u00a7\u00c3\u00a3o e que tem potencial de prejudicar a modelagem).</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nfig, axes = plt.subplots(figsize=(15, 55))\nsns.set()\nfor i in range(1, 34):\n    plt.subplot(12, 3, i)\n    sns.distplot(numeric_features[numeric_features_list[i-1]].dropna())\nplt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nfig, axes = plt.subplots(figsize=(22, 60))\nsns.set()\nfor i in range(1, 34):\n    plt.subplot(12, 3, i)\n    sns.scatterplot(y='SalePrice', x=numeric_features[numeric_features_list[i-1]], data=train_dataset)\nplt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Observando os gr\u00c3\u00a1ficos acima, decide-se por retirar colunas que possuem massiva concentra\u00c3\u00a7\u00c3\u00a3o de zeros, pois se a vari\u00c3\u00a1vel tem comportamento constante, n\u00c3\u00a3o influenciar\u00c3\u00a1 no modelo.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset = train_dataset.drop(['BsmtFinSF2', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'LowQualFinSF', 'KitchenAbvGr'], axis=1)\ntest_dataset = test_dataset.drop(['BsmtFinSF2', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'LowQualFinSF', 'KitchenAbvGr'], axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nnumeric_features = numeric_features.drop(['BsmtFinSF2', '3SsnPorch', 'ScreenPorch','PoolArea', 'MiscVal', 'LowQualFinSF', 'KitchenAbvGr'], axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Observa-se em escala maior algumas vari\u00c3\u00a1veis em que se faz necess\u00c3\u00a1ria a remo\u00c3\u00a7\u00c3\u00a3o de <em>outliers</em>.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h5 id=\"TotalBsmt\">TotalBsmt<a class=\"anchor-link\" href=\"#TotalBsmt\">\u00b6</a></h5>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsns.scatterplot(y='SalePrice', x='TotalBsmtSF', data=train_dataset)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h5 id=\"LotArea\">LotArea<a class=\"anchor-link\" href=\"#LotArea\">\u00b6</a></h5>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsns.scatterplot(y='SalePrice', x='LotArea', data=train_dataset)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h5 id=\"GrLivArea\">GrLivArea<a class=\"anchor-link\" href=\"#GrLivArea\">\u00b6</a></h5>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsns.scatterplot(y='SalePrice', x='GrLivArea', data=train_dataset);\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h5 id=\"GarageArea\">GarageArea<a class=\"anchor-link\" href=\"#GarageArea\">\u00b6</a></h5>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsns.scatterplot(y='SalePrice', x='GarageArea', data=train_dataset);\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Remove-se ent\u00c3\u00a3o, os outliers com as opera\u00c3\u00a7\u00c3\u00b5es abaixo:</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset = train_dataset[train_dataset['TotalBsmtSF'] < 2500]\ntrain_dataset = train_dataset[train_dataset['LotArea'] < 100000]\ntrain_dataset = train_dataset[train_dataset['GrLivArea'] < 3000]\ntrain_dataset = train_dataset[train_dataset['GarageArea'] < 1000]\ntrain_dataset = train_dataset[train_dataset['1stFlrSF'] < 2300]\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>A etapa abaixo \u00c3\u00a9 uma parte do trabalho em que n\u00c3\u00a3o houve sucesso. Se buscou transformar alguns preditores para aproxim\u00c3\u00a1-los tamb\u00c3\u00a9m da gaussiana. As transforma\u00c3\u00a7\u00c3\u00b5es se resumiram as colunas com assimetria maior que 75%. Utilizou-se a transforma\u00c3\u00a7\u00c3\u00a3o de Box-cox, com lambda escolhido empiricamente. Por\u00c3\u00a9m, as previs\u00c3\u00b5es do modelo com tais medi\u00c3\u00a7\u00c3\u00b5es geraram erros maiores quando submetidas no Kaggle.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nskewed_feats = numeric_features.apply(lambda x: x.dropna().skew()).sort_values(ascending=False)\nprint(\"\\nSkew in features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nskewed_features = list(skewness[skewness['Skew'] > 0.75].index)\nlmbda = 0.15\nprint (\"Feature\", \"\\t\", \"Original Skewness\", \"\\t\", \"Skewness after Box-cox transformation\\n\")\nfor feat in skewed_features:\n    print (feat, \"\\t\", numeric_features[feat].skew(), \"\\t\", (boxcox1p(numeric_features[feat], lmbda)).skew())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Como alguns colunas se distanciaram a distribui\u00c3\u00a7\u00c3\u00a3o normal, estas foram retiradas da transforma\u00c3\u00a7\u00c3\u00a3o, pois entende-se que a transforma\u00c3\u00a7\u00c3\u00a3o, nestes casos, prejudicava o modelo.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nfor col in ['BsmtHalfBath', 'EnclosedPorch', 'TotalBsmtSF', 'BsmtUnfSF']:\n    skewed_features.remove(col)\n\nfor feat in skewed_features:\n#    train_dataset[feat] = boxcox1p(numeric_features[feat], lmbda)\n#    test_dataset[feat] = boxcox1p(numeric_features[feat], lmbda)\n    print (feat, \"\\t\", numeric_features[feat].skew(), \"\\t\", (boxcox1p(numeric_features[feat], lmbda)).skew())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Retornando a coluna \"MSSubClass\", que deve ser transformada em coluna de dados categ\u00c3\u00b3ricos para ser tratada corretamente. Foram criadas siglas de acordo com a descri\u00c3\u00a7\u00c3\u00a3o do dataset para cada valor num\u00c3\u00a9rico. Cada sigla agora \u00c3\u00a9 uma categoria de MSSubClass.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n#20 \t1-STORY 1946 & NEWER ALL STYLES  \t(NAL)\n#30 \t1-STORY 1945 & OLDER  \t(OLD)\n#40 \t1-STORY W/FINISHED ATTIC ALL AGES  \t(SFAAT)\n#45 \t1-1/2 STORY - UNFINISHED ALL AGES  \t(SUAA)\n#50 \t1-1/2 STORY FINISHED ALL AGES (SFAA)\n#60 \t2-STORY 1946 & NEWER  \t(SN)\n#70 \t2-STORY 1945 & OLDER  \t(SO)\n#75 \t2-1/2 STORY ALL AGES  \t(SAG)\n#80 \tSPLIT OR MULTI-LEVEL  \t(SML)\n#85 \tSPLIT FOYER  \t(SF)\n#90 \tDUPLEX - ALL STYLES AND AGES  \t(DASA)\n#120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER  \t(OneSPUD)\n#150\t1-1/2 STORY PUD - ALL AGES  \t(HalfSPUD)\n#160\t2-STORY PUD - 1946 & NEWER  \t(TwoSPUD)\n#180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER  \t(PUDMultilvl)\n#190\t2 FAMILY CONVERSION - ALL STYLES AND AGES \t(FamConv)\n\ndef categorize_MSSubClass(x):\n    if x == 20:\n        return 'NAL'\n    if x == 30:\n        return 'OLD'\n    if x == 40:\n        return 'SFAAT'\n    if x == 45:\n        return 'SUAA'\n    if x == 50:\n        return 'SFAA'\n    if x == 60:\n        return 'SN'\n    if x == 70:\n        return 'SO'\n    if x == 80:\n        return 'SML'\n    if x == 85:\n        return 'SF'\n    if x == 90:\n        return 'DASA'\n    if x == 120:\n        return 'OneSPUD'\n    if x == 150:\n        return 'HalfSPUD'\n    if x == 160:\n        return 'TwoSPUD'\n    if x == 180:\n        return 'PUDMultilvl'\n    if x == 190:\n        return 'FamConv'\n    \ntrain_dataset['cat_MSSubClass'] = train_dataset.MSSubClass.apply(categorize_MSSubClass)\ntest_dataset['cat_MSSubClass'] = test_dataset.MSSubClass.apply(categorize_MSSubClass)\n\ntest_dataset['cat_MSSubClass'].value_counts()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>A sa\u00c3\u00adda acima confirma a transforma\u00c3\u00a7\u00c3\u00a3o, com a crian\u00c3\u00a7\u00c3\u00a3o da nova coluna \"cat_MSSubClass\".</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h3 id=\"Lidando-com-vari\u00e1veis-categ\u00f3ricas\">Lidando com vari\u00e1veis categ\u00f3ricas<a class=\"anchor-link\" href=\"#Lidando-com-vari\u00e1veis-categ\u00f3ricas\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Em suma, apenas observa-se o comportamento das vari\u00c3\u00a1veis e estuda-se como mape\u00c3\u00a1-las em valores num\u00c3\u00a9ricos.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ncategoric_features = train_dataset.select_dtypes(include='object')\ncategoric_features_list = categoric_features.columns\nprint (\"Shape:\", categoric_features.shape, '\\n')\nprint (\"Estas s\u00c3\u00a3o as colunas categ\u00c3\u00b3ricas: \", categoric_features.columns,\"\\n\")\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Na c\u00c3\u00a9lula abaixo, adiciona-se a categoria \"MISSING\" para que seja poss\u00c3\u00advel gerar gr\u00c3\u00a1ficos de todas as vari\u00c3\u00a1veis caateg\u00c3\u00b3ricas, inclusive as que possuem valores NaN.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nfor feat in categoric_features_list:\n    categoric_features[feat] = categoric_features[feat].astype('category')\n    if categoric_features[feat].isnull().any():\n        categoric_features[feat] = categoric_features[feat].cat.add_categories(['MISSING'], inplace=False)\n        categoric_features[feat] = categoric_features[feat].fillna('MISSING')\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\nf = pd.melt(train_dataset, id_vars = ['SalePrice'], value_vars=categoric_features_list)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Observa-se que a coluna \"SaleCondition\" em especial, existe um valor('Partial') que destoa do restante, possuindo uma rela\u00c3\u00a7\u00c3\u00a3o com pre\u00c3\u00a7os mais elevados. Portanto, decide-se codificar a coluna como se segue, convertendo os valores \"Partial\" em '1's e o restantes em zeros.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndef encode(x): return 1 if x == 'Partial' else 0\ntrain_dataset['enc_SaleCondition'] = train_dataset.SaleCondition.apply(encode)\ntest_dataset['enc_SaleCondition'] = test_dataset.SaleCondition.apply(encode)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>J\u00c3\u00a1 no caso de Street, obeserva-se que a mesma \u00c3\u00a9 uma vari\u00c3\u00a1vel bin\u00c3\u00a1ria (a rua \u00c3\u00a9 pavimentada ou n\u00c3\u00a3o). Portanto, codifica-se como se segue:</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset['enc_street'] = pd.get_dummies(train_dataset.Street, drop_first=True)\ntest_dataset['enc_street'] = pd.get_dummies(test_dataset.Street, drop_first=True)\n\ntrain_dataset['enc_street'].value_counts()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Pode-se realizar o mesmo procedimento para \"Utilities\" (uma vari\u00c3\u00a1vel que indica as utilidades suportadas pela casa: g\u00c3\u00a1s, eletricidade e \u00c3\u00a1gua). Por\u00c3\u00a9m, observando a distribui\u00c3\u00a7\u00c3\u00a3o de seus valores, chega-se a conclus\u00c3\u00a3o que \u00c3\u00a9 uma coluna descat\u00c3\u00a1vel.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nprint(train_dataset['Utilities'].value_counts(), '\\n')\nprint(test_dataset['Utilities'].value_counts())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Nota-se que, no dataset de treino s\u00c3\u00b3 h\u00c3\u00a1 um valor \"NoSeWa\"(Eletricidade e G\u00c3\u00a1s apenas). No dataset de teste s\u00c3\u00b3 h\u00c3\u00a1 casa com todas as utilidades (AllPub). Portanto, 'Utilities' pode ser descartado.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ntrain_dataset = train_dataset.drop(['Utilities'], axis = 1)\ntest_dataset = test_dataset.drop(['Utilities'], axis = 1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nprint(train_dataset.shape)\nprint(test_dataset.shape)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Criamos colunas bin\u00c3\u00a1rias para o restante das colunas categ\u00c3\u00b3ricas, mapeando cada categoria em uma nova coluna bin\u00c3\u00a1ria.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nohe_train_dataset = pd.get_dummies(train_dataset)\nohe_test_dataset = pd.get_dummies(test_dataset)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nprint(ohe_train_dataset.shape)\nprint(ohe_test_dataset.shape)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>O Comando abaixo alinhas os dois datasets, para que regressor receba as mesmas colunas e na mesma ordem nos dois casos.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<h3 id=\"Ajuste-final\">Ajuste final<a class=\"anchor-link\" href=\"#Ajuste-final\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Por fim, iremos remover todas as colunas que possuem correla\u00c3\u00a7\u00c3\u00a3o menor que 7% com a vari\u00c3\u00a1vel alvo. Este valor foi escolhido empiricamente.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndrop_list = []\nfor key, value in ohe_train_dataset.corr()['SalePrice'].items():\n    if abs(value) < 0.07 and key != \"Id\":\n        drop_list.append(key)\n\nfinal_train_dataset = ohe_train_dataset.drop(drop_list, axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ny = np.log(final_train_dataset.SalePrice)\nX = final_train_dataset.drop(['SalePrice'], axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nX_train, X_test = X.align(ohe_test_dataset, join='left', axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nprint(X_train.shape)\nprint(X_test.shape)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Construindo-o-modelo\">Construindo o modelo<a class=\"anchor-link\" href=\"#Construindo-o-modelo\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Representamos os preditores usados para regress\u00c3\u00a3o como o vetor X e a vari\u00c3\u00a1vel alvo como y.</p>\n<p>Utilizaremos o regressor ElasticNet, pois este foi o que apresentou melhor resultado. O segundo melhor resultado foi obtido com o regressor linear. Tamb\u00c3\u00a9m foram testados os regressores Floresta Rand\u00c3\u00b4mica e Gradient Boosting, que apresentaram resultados mais distantes dos dois primeiros.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nmy_pipeline = make_pipeline(Imputer(), ElasticNet(alpha=0.0005, l1_ratio=.9))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Retirando a coluna 'Id' de X_train para o teste de valida\u00c3\u00a7\u00c3\u00a3o cruzada. A coluna 'Id' ser\u00c3\u00a1 retirada de X_test algumas c\u00c3\u00a9lulas abaixo.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nX_train = X_train.drop(['Id'], axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Testando-o-modelo-com-valida\u00e7\u00e3o-cruzada-e-RMSE-e-MAE\">Testando o modelo com valida\u00e7\u00e3o cruzada e RMSE e MAE<a class=\"anchor-link\" href=\"#Testando-o-modelo-com-valida\u00e7\u00e3o-cruzada-e-RMSE-e-MAE\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>A fun\u00c3\u00a7\u00c3\u00a3o cross_val_score do scikit-learn trabalha apenas com erro quadr\u00c3\u00a1tico m\u00c3\u00a9dio negativo, assim como erro absoluto m\u00c3\u00a9dio negativo. Portanto, para termos o RMSE, \u00c3\u00a9 preciso realizar algumas opera\u00c3\u00a7\u00c3\u00b5es matem\u00c3\u00a1ticas.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nscores = [sqrt(-1*x) for x in cross_val_score(my_pipeline, X_train, y, scoring='neg_mean_squared_error')]\nprint(scores,'\\n')\n\nprint (\"M\u00c3\u00a9dia:\", np.mean(scores))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>O erro absoluto m\u00c3\u00a9dio \u00c3\u00a9 calculado abaixo:</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nscores = cross_val_score(my_pipeline, X_train, y, scoring='neg_mean_absolute_error')\nprint(scores*(-1),'\\n')\n\nprint (\"M\u00c3\u00a9dia:\", np.mean(scores)*(-1))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h3 id=\"Fazendo-as-previs\u00f5es-com-o-arquivo-de-teste-e-gerando-o-arquivo-csv-para-submiss\u00e3o\">Fazendo as previs\u00f5es com o arquivo de teste e gerando o arquivo csv para submiss\u00e3o<a class=\"anchor-link\" href=\"#Fazendo-as-previs\u00f5es-com-o-arquivo-de-teste-e-gerando-o-arquivo-csv-para-submiss\u00e3o\">\u00b6</a></h3>\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n<p>Criando o dataset \"submission\" que ter\u00c3\u00a1 apenas duas colunas: \"Id\" e \"SalePrice\"</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsubmission = pd.DataFrame()\nsubmission['Id'] = X_test.Id\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Retirando a coluna \"Id\" do dataset de teste para realizar as previs\u00c3\u00b5es.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nX_test = X_test.drop(['Id'], axis=1)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Preenchendo os campos vazios do dataset de teste.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nmy_imputer = Imputer()\nmy_imputer.fit_transform(X_train)\nfinal_test = my_imputer.transform(X_test)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Gerando as previs\u00c3\u00b5es</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nmy_pipeline.fit(X_train, y)\npredictions = my_pipeline.predict(final_test)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Aplicando a exponencial em 'SalePrice' para anular a transforma\u00c3\u00a7\u00c3\u00a3o feita anteriormente.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nfinal_predictions = np.exp(predictions)\nsubmission['SalePrice'] = final_predictions\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Finalmente, na c\u00c3\u00a9lula abaixo, gera-se o arquivo que ser\u00c3\u00a1 submetido no Kaggle.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nsubmission.to_csv('submission.csv', index=False)\n\n"], "execution_count": null, "cell_type": "code"}], "metadata": {}}